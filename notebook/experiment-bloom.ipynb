{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade transformers\n!pip install --upgrade datasets\n!pip install --upgrade accelerate\n!pip install -i https://test.pypi.org/simple/ bitsandbytes\n!pip install peft\n!pip install rouge_score\n!pip install --upgrade evaluate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-16T17:21:54.745316Z","iopub.execute_input":"2023-07-16T17:21:54.745633Z","iopub.status.idle":"2023-07-16T17:23:31.176538Z","shell.execute_reply.started":"2023-07-16T17:21:54.745604Z","shell.execute_reply":"2023-07-16T17:23:31.175314Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.30.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.65.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nCollecting datasets\n  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.23.5)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (1.5.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.65.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.5.7)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\nInstalling collected packages: datasets\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\nSuccessfully installed datasets-2.13.1\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.20.3)\nCollecting accelerate\n  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.20.3\n    Uninstalling accelerate-0.20.3:\n      Successfully uninstalled accelerate-0.20.3\nSuccessfully installed accelerate-0.21.0\nLooking in indexes: https://test.pypi.org/simple/\nCollecting bitsandbytes\n  Downloading https://test-files.pythonhosted.org/packages/5c/e0/597d593ec3b6cf5ea7eb4894a545045bd95611de8a316a2a1eaa838a2459/bitsandbytes-0.39.0-py3-none-any.whl (95.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.39.0\nCollecting peft\n  Downloading peft-0.3.0-py3-none-any.whl (56 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.0.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.30.2)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from peft) (0.21.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.16.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.3.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (4.65.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->peft) (2023.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft) (2023.5.7)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nInstalling collected packages: peft\nSuccessfully installed peft-0.3.0\nCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.23.5)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24954 sha256=c1a44ef35080ab15ee5e20a2b94032b4434ebf5e717ffe98e14e3cf531481ebc\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\nCollecting evaluate\n  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.23.5)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.5.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.65.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.6.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.5.7)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\nInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import transformers\nfrom transformers import  BloomForCausalLM, AutoTokenizer\nfrom transformers import get_scheduler\n\nimport datasets\nfrom datasets import load_dataset\n\nimport torch\nfrom torch.optim import AdamW\nfrom typing import Union\n\nimport matplotlib.pyplot as plt\n\nfrom contextlib import nullcontext\nfrom torch.cuda.amp import GradScaler, autocast\n\nimport evaluate\n\nfrom peft import LoraConfig, get_peft_model, PeftConfig, PeftModel\n\nfrom tqdm.auto import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-07-16T17:23:31.179194Z","iopub.execute_input":"2023-07-16T17:23:31.179588Z","iopub.status.idle":"2023-07-16T17:23:46.042328Z","shell.execute_reply.started":"2023-07-16T17:23:31.179548Z","shell.execute_reply":"2023-07-16T17:23:46.041338Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\nCUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\nCUDA SETUP: Highest compute capability among GPUs detected: 6.0\nCUDA SETUP: Detected CUDA version 118\nCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib'), PosixPath('/usr/local/lib/x86_64-linux-gnu'), PosixPath('/usr/local/nvidia/lib')}\n  warn(msg)\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n  warn(msg)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-07-16T17:23:46.044703Z","iopub.execute_input":"2023-07-16T17:23:46.045973Z","iopub.status.idle":"2023-07-16T17:23:46.051233Z","shell.execute_reply.started":"2023-07-16T17:23:46.045936Z","shell.execute_reply":"2023-07-16T17:23:46.050220Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"mixed_precision_dtype = torch.float16 if device.type == \"cuda\" else torch.bfloat16\nctx = torch.amp.autocast(device_type = device.type, dtype=mixed_precision_dtype)\nscaler = GradScaler()","metadata":{"execution":{"iopub.status.busy":"2023-07-16T17:23:46.054347Z","iopub.execute_input":"2023-07-16T17:23:46.054865Z","iopub.status.idle":"2023-07-16T17:23:46.065593Z","shell.execute_reply.started":"2023-07-16T17:23:46.054822Z","shell.execute_reply":"2023-07-16T17:23:46.064638Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# **Build Modules**","metadata":{}},{"cell_type":"markdown","source":"### **Config tokenizer and model**","metadata":{}},{"cell_type":"code","source":"class Config:\n    def __init__(self,\n                 device):\n        self.device = device\n    \n    def tokenizer(self, model_checkpoint):\n        tok = AutoTokenizer.from_pretrained(model_checkpoint)\n        return tok\n    \n    def load_pretrained_model(self, model_checkpoint):\n        model = BloomForCausalLM.from_pretrained(model_checkpoint)\n        return model.to(self.device)\n    \n    def add_lora(self, model, r: int, lora_alpha: int, lora_dropout: float):\n        lora_config = LoraConfig(r = r,\n                                 lora_alpha = lora_alpha,\n                                 lora_dropout = lora_dropout,\n                                 bias = \"none\",\n                                 task_type = \"CAUSAL_LM\")\n        lora_model = get_peft_model(model, lora_config)\n        return lora_model","metadata":{"execution":{"iopub.status.busy":"2023-07-16T17:23:46.066977Z","iopub.execute_input":"2023-07-16T17:23:46.067481Z","iopub.status.idle":"2023-07-16T17:23:46.077478Z","shell.execute_reply.started":"2023-07-16T17:23:46.067448Z","shell.execute_reply":"2023-07-16T17:23:46.076443Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### **Create prompt**","metadata":{}},{"cell_type":"code","source":"class Prompter(object):\n    __slots__ = (\"template\")\n\n    def __init__(self, template_name: str = \"\", verbose: bool = False):\n        self.template = {\n            \"prompt_input\": \"Dưới đây là một Instruction mô tả một nhiệm vụ, được ghép nối với một Input cung cấp thêm ngữ cảnh. Viết một Response hoàn thành yêu cầu một cách thích hợp.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\",\n            \"prompt_no_input\": \"Dưới đây là một Instruction mô tả một nhiệm vụ. Việt một Response hoàn thành yêu cầu một cách thích hợp.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n\",\n            \"response_split\": \"### Response:\"\n        }\n\n    def generate_prompt(\n        self,\n        instruction: str,\n        input: Union[None, str] = None,\n        label: Union[None, str] = None,\n    ) -> str:\n        # returns the full prompt from instruction and optional input\n        # if a label (=response, =output) is provided, it's also appended.\n        if input:\n            res = self.template[\"prompt_input\"].format(\n                instruction=instruction, input=input\n            )\n        else:\n            res = self.template[\"prompt_no_input\"].format(\n                instruction=instruction\n            )\n        if label:\n            res = f\"{res}{label}\"\n        return res\n\n    def get_response(self, output: str) -> str:\n        return output.split(self.template[\"response_split\"])[1].strip()","metadata":{"execution":{"iopub.status.busy":"2023-07-16T17:23:46.079520Z","iopub.execute_input":"2023-07-16T17:23:46.079780Z","iopub.status.idle":"2023-07-16T17:23:46.091976Z","shell.execute_reply.started":"2023-07-16T17:23:46.079757Z","shell.execute_reply":"2023-07-16T17:23:46.091028Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### **Data processing and analysis**","metadata":{}},{"cell_type":"code","source":"class DataProcess:\n    def __init__(self,\n                 data_path,\n                 tokenizer):\n        self.data_path = data_path\n        self.tokenizer = tokenizer\n        \n    def load_data(self):\n        dataset = load_dataset(self.data_path, \"vi\", split = \"train\")\n        return dataset\n    \n    def statistical(self, dataset, prompter):\n        prompt_len = []\n        for line in dataset:\n            full_prompt = prompter.generate_prompt(line[\"instruction\"],\n                                                   line[\"input\"],\n                                                   line[\"output\"])\n            prompt_len.append(len(self.tokenizer.encode(full_prompt)))\n        return prompt_len\n    \n    def draw(self, prompt_len):\n        freq = {}\n        for num in prompt_len:\n            if num in freq:\n                freq[num] += 1\n            else:\n                freq[num] = 1\n        \n        max_freq = 0\n        max_keys = None\n\n        for k in freq.keys():\n            if freq[k] >= max_freq:\n                max_freq = freq[k]\n                max_keys = k\n                \n        fig = plt.figure(figsize = ((8, 5)))\n        plt.bar(freq.keys(), freq.values(), width = 0.6)   \n        plt.xlabel(\"Prompt length\")\n        plt.ylabel(\"Prompt length frequency\")\n        plt.show()\n        \n        print(\"Length occupies the most frequency: \", max_keys)\n        print(\"Maximum frequency: \", max_freq)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T17:23:46.093380Z","iopub.execute_input":"2023-07-16T17:23:46.093732Z","iopub.status.idle":"2023-07-16T17:23:46.106188Z","shell.execute_reply.started":"2023-07-16T17:23:46.093696Z","shell.execute_reply":"2023-07-16T17:23:46.105202Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### **Create input for the model**","metadata":{}},{"cell_type":"code","source":"class MODEL_INPUTS:\n    def __init__(self,\n                 prompter,\n                 tokenizer,\n                 max_length: int):\n        self.prompter = prompter\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def tokenize(self, prompt, add_eos_token = True):\n        result = self.tokenizer(prompt,\n                                truncation = True,\n                                max_length = self.max_length,\n                                padding = True,\n                                return_tensors = None)\n        if (   \n            result[\"input_ids\"][-1] != self.tokenizer.eos_token_id\n            and len(result[\"input_ids\"]) < self.max_length\n            and add_eos_token\n        ):\n            \n            result[\"input_ids\"].append(self.tokenizer.eos_token_id)\n            result[\"attention_mask\"].append(1)\n\n        result[\"labels\"] = result[\"input_ids\"].copy()\n        return result\n        \n    def generate_and_tokenize_prompt(self, dataset):\n        full_prompt = self.prompter.generate_prompt(dataset[\"instruction\"],\n                                                    dataset[\"input\"],\n                                                    dataset[\"output\"])\n        \n        tokenized_full_prompt = self.tokenize(full_prompt)\n        return tokenized_full_prompt\n    \n    def prepare_dataloader(self,\n                           train_data,\n                           valid_data,\n                           batch_size: int):\n        \n        train_dataloader = torch.utils.data.DataLoader(dataset = train_data,\n                                                       batch_size = batch_size,\n                                                       collate_fn = transformers.DataCollatorForSeq2Seq(tokenizer = self.tokenizer,\n                                                                                                        padding = True,\n                                                                                                        return_tensors = \"pt\"))\n        valid_dataloader = torch.utils.data.DataLoader(dataset = valid_data,\n                                                       batch_size = batch_size,\n                                                       collate_fn = transformers.DataCollatorForSeq2Seq(tokenizer = self.tokenizer,\n                                                                                                        padding = True,\n                                                                                                        return_tensors = \"pt\"))\n        return train_dataloader, valid_dataloader","metadata":{"execution":{"iopub.status.busy":"2023-07-16T17:23:46.107561Z","iopub.execute_input":"2023-07-16T17:23:46.108039Z","iopub.status.idle":"2023-07-16T17:23:46.120974Z","shell.execute_reply.started":"2023-07-16T17:23:46.108007Z","shell.execute_reply":"2023-07-16T17:23:46.120066Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### **Evaluate and Test**","metadata":{}},{"cell_type":"code","source":"class EVALUATEandTEST:\n    def __init__(self,\n                 tokenizer,\n                 device,\n                 metrics,\n                 prompter,\n                 ctx):\n        self.tokenizer = tokenizer\n        self.device = device\n        self.metrics = metrics\n        self.prompter = prompter\n        self.ctx = ctx\n        \n    def evaluate(self,\n                 model,\n                 dataset,\n                 gen_mode: bool = False,\n                 samples_gen: int = None,\n                 samples_eval: int = None):\n        model.eval()\n        total_loss = 0\n        predicted_texts, correct_texts = [], []\n        current_gen_mode = gen_mode\n        for i, batch in enumerate(dataset):\n            batch = {k:v.to(self.device) for k, v in batch.items()}\n            with torch.no_grad():\n                with self.ctx:\n                    outputs = model(input_ids = batch[\"input_ids\"],\n                                    attention_mask = batch[\"attention_mask\"],\n                                    labels = batch[\"labels\"],\n                                    return_dict = True)\n            loss = outputs.loss\n            total_loss += loss.item()\n            \n            if current_gen_mode is True:\n                outputs_gen = model.generate(input_ids = batch[\"input_ids\"],\n                                             attention_mask = batch[\"attention_mask\"],\n                                             top_k = 40,\n                                             no_repeat_ngram_size = 3,\n                                             num_beams = 1,\n                                             max_new_tokens = 256,\n                                             bos_token_id = self.tokenizer.bos_token_id,\n                                             eos_token_id = self.tokenizer.eos_token_id,\n                                             pad_token_id = self.tokenizer.pad_token_id,\n                                             early_stopping = True)\n                \n                generate_batch = self.tokenizer.batch_decode(outputs_gen, skip_special_tokens = True)\n                correct_batch = self.tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens = True)\n                \n                for j in len(generate_batch):       \n                    prompt = generate_batch[j]\n                    response = self.prompter.get_response(prompt)\n                    generate_batch[j] = response\n                        \n                for k in len(correct_batch):\n                    prompt = correct_batch[k]\n                    response = self.prompter.get_response(prompt)\n                    correct_batch[k] = response\n                    \n                predicted_texts += generate_batch\n                correct_texts += correct_batch\n                \n            if samples_gen is not None:\n                if i >= samples_gen:\n                    current_gen_mode = False\n            \n            if samples_eval is not None:\n                if i >= samples_eval:\n                    break\n                \n        if gen_mode is True:\n            rouge = self.metrics.compute(predictions = predicted_texts,\n                                         references = correct_texts)\n        \n            return {\"rouge1\": rouge[\"rouge1\"],\n                    \"rouge2\": rouge[\"rouge2\"],\n                    \"rougeL\": rouge[\"rougeL\"],\n                    \"rougeLsum\": rouge[\"rougeLsum\"],\n                    \"loss\": total_loss/(samples_eval + 1 if samples_eval is not None else len(dataset))}\n     \n        else:\n            return {\"loss\": total_loss/(samples_eval + 1 if samples_eval is not None else len(dataset))}\n    \n    def test(self,\n             model,\n             instruction: str,\n             input: str = None,\n             label: str = None):\n        \n        prompt = self.prompter.generate_prompt(instruction = instruction, input = input)\n        inputs = self.tokenizer(prompt, return_tensors = \"pt\")\n        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n        outputs = model.generate(input_ids = inputs[\"input_ids\"],\n                                 attention_mask = inputs[\"attention_mask\"],\n                                 max_new_tokens = 256,\n                                 no_repeat_ngram_size = 3,\n                                 num_beams = 1,\n                                 top_k = 40,\n                                 bos_token_id = self.tokenizer.bos_token_id,\n                                 eos_token_id = self.tokenizer.eos_token_id,\n                                 pad_token_id = self.tokenizer.pad_token_id,\n                                 early_stopping = True)\n        text = self.tokenizer.batch_decode(outputs, skip_special_tokens = True)[0]\n        response = self.prompter.get_response(text)\n        if label is not None:\n            return {\"label\": label,\n                    \"response\": response}\n        else:\n            return {\"response\": response}","metadata":{"execution":{"iopub.status.busy":"2023-07-16T17:23:46.124363Z","iopub.execute_input":"2023-07-16T17:23:46.124660Z","iopub.status.idle":"2023-07-16T17:23:46.146860Z","shell.execute_reply.started":"2023-07-16T17:23:46.124625Z","shell.execute_reply":"2023-07-16T17:23:46.145940Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### **Train and Save**","metadata":{}},{"cell_type":"code","source":"class Trainer:\n    def __init__(self,\n                 lr: float,\n                 epochs: int,\n                 model,\n                 gradient_accumulation_steps: int,\n                 device,\n                 evaluate_fn,\n                 mixed_precision_dtype,\n                 scaler,\n                 ctx):\n        self.epochs = epochs\n        self.model = model\n        self.optimizer = AdamW(model.parameters(), lr = lr)\n        self.gradient_accumulation_steps = gradient_accumulation_steps\n        self.device = device\n        self._eval = evaluate_fn\n        self.mixed_precision_dtype = mixed_precision_dtype\n        self.scaler = scaler\n        self.ctx = ctx\n        \n    def train(self,\n              train_dataloader, \n              display_steps: int,\n              save_steps: int,\n              save_name: str = None,\n              save_checkpoint: bool = False,\n              valid_dataloader = None,\n              samples_gen: int = None,\n              samples_eval: int = None,\n              gen_mode: bool = False,\n              checkpoint = None):\n        \n        num_update_steps_per_epoch = len(train_dataloader)\n        \n        if checkpoint is not None:\n            current_steps = checkpoint[\"current_steps\"]\n            self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n            num_steps = num_update_steps_per_epoch * self.epochs - current_steps\n            progress_bar = tqdm(range(num_steps))\n            lr_scheduler = get_scheduler(\"cosine\",\n                                         optimizer = self.optimizer,\n                                         num_warmup_steps = 100,\n                                         num_training_steps = num_steps)\n            lr_scheduler.load_state_dict(checkpoint[\"lr_scheduler_state_dict\"])\n            self.scaler.load_state_dict(checkpoint[\"scaler_state_dict\"])\n            self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n            total_loss = checkpoint[\"total_loss\"]\n            \n        else:\n            current_steps = 0\n            num_steps = num_update_steps_per_epoch * self.epochs\n            progress_bar = tqdm(range(num_steps))\n            lr_scheduler = get_scheduler(\"cosine\",\n                                         optimizer = self.optimizer,\n                                         num_warmup_steps = 100,\n                                         num_training_steps = num_steps)\n            total_loss = 0\n            \n        idx = 0\n        for epoch in range(self.epochs):\n            \n            self.model.train()\n            for batch in train_dataloader:\n                idx += 1\n                if idx > current_steps:\n                    batch = {k:v.to(self.device) for k, v in batch.items()}\n                    self.optimizer.zero_grad()\n                    with self.ctx:\n                        outputs = self.model(input_ids = batch[\"input_ids\"],\n                                             attention_mask = batch[\"attention_mask\"],\n                                             labels = batch[\"labels\"],\n                                             return_dict = True)\n                    loss = outputs.loss\n                    total_loss += loss.item()\n                    \n                    loss /= self.gradient_accumulation_steps\n                    if self.mixed_precision_dtype:\n                        self.scaler.scale(loss).backward()\n                        \n                        if idx % self.gradient_accumulation_steps == 0:\n                            self.scaler.step(self.optimizer)\n                            lr_scheduler.step()\n                            self.scaler.update()\n                            \n                    else:\n                        loss.backward()\n                        if idx % self.gradient_accumulation_steps == 0:\n                            self.optimizer.step()\n                            lr_scheduler.step()\n                    \n                    progress_bar.update(1)\n                    current_steps += 1\n                    \n                    if current_steps % display_steps == 0:\n                        if current_steps % len(train_dataloader) == 0:\n                            if valid_dataloader is not None:\n                                eval_ = self._eval(self.model, valid_dataloader, samples_gen = samples_gen, samples_eval = samples_eval, gen_mode = True)\n                                print(f'Epoch: {epoch + 1} -- step: {current_steps} -- train_loss: {total_loss/current_steps} -- val_loss: {eval_[\"loss\"]}')\n                                print(f'rouge1: {eval_[\"rouge1\"]} -- rouge2: {eval_[\"rouge2\"]} -- rougeL: {eval_[\"rougeL\"]} -- rougeLsum: {eval_[\"rougeLsum\"]}')\n                                print(\"----------------------- End of epoch {} -----------------------\".format(epoch + 1))\n                                \n                            else:\n                                print(f'Epoch: {epoch + 1} -- step: {current_steps} -- train_loss: {total_loss/current_steps}') \n                                print(\"----------------------- End of epoch {} -----------------------\".format(epoch + 1))\n                        else:\n                            if valid_dataloader is not None:\n                                eval_ = self._eval(self.model, valid_dataloader, samples_eval = samples_eval, gen_mode = False)\n                                print(f'Epoch: {epoch + 1} -- step: {current_steps} -- train_loss: {total_loss/current_steps} -- val_loss: {eval_[\"loss\"]}')\n                            else:\n                                print(f'Epoch: {epoch + 1} -- step: {current_steps} -- train_loss: {total_loss/current_steps}')\n                    \n                    if save_checkpoint is True:\n                        if current_steps % save_steps == 0:\n                            print(\"Saving..........\")\n                            torch.save({\"model_state_dict\": self.model.state_dict(),\n                                        \"optimizer_state_dict\": self.optimizer.state_dict(),\n                                        \"scaler_state_dict\": self.scaler.state_dict(),\n                                        \"lr_scheduler_state_dict\": lr_scheduler.state_dict(),\n                                        \"current_steps\": current_steps,\n                                        \"total_loss\": total_loss},\n                                       save_name)\n                            print(\"****** Save successfully ******\")\n                    ","metadata":{"execution":{"iopub.status.busy":"2023-07-16T17:23:46.150900Z","iopub.execute_input":"2023-07-16T17:23:46.151390Z","iopub.status.idle":"2023-07-16T17:23:46.176422Z","shell.execute_reply.started":"2023-07-16T17:23:46.151363Z","shell.execute_reply":"2023-07-16T17:23:46.175411Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# **Run Modules**","metadata":{}},{"cell_type":"code","source":"config = Config(device)\ntokenizer = config.tokenizer(model_checkpoint = \"bigscience/bloom\")","metadata":{"execution":{"iopub.status.busy":"2023-07-16T17:23:46.177505Z","iopub.execute_input":"2023-07-16T17:23:46.177763Z","iopub.status.idle":"2023-07-16T17:23:49.335281Z","shell.execute_reply.started":"2023-07-16T17:23:46.177740Z","shell.execute_reply":"2023-07-16T17:23:49.334266Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c434a08e589432b92c78e21366607dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"129a36af47344422b0981e50ef026f33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29206fc15bd948adae4a79e574fe4a80"}},"metadata":{}}]},{"cell_type":"code","source":"model = config.load_pretrained_model(model_checkpoint = \"bigscience/bloom-560m\")","metadata":{"execution":{"iopub.status.busy":"2023-07-16T17:23:49.336929Z","iopub.execute_input":"2023-07-16T17:23:49.337332Z","iopub.status.idle":"2023-07-16T17:24:57.903223Z","shell.execute_reply.started":"2023-07-16T17:23:49.337296Z","shell.execute_reply":"2023-07-16T17:24:57.902124Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/693 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48430a46fc9f435f96ed2a9621658aaf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32ebb528c07f42e6bfef60e6f6cba507"}},"metadata":{}}]},{"cell_type":"code","source":"lora_model = config.add_lora(model = model, r = 8, lora_alpha = 16, lora_dropout = 0.05)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T17:25:31.621360Z","iopub.execute_input":"2023-07-16T17:25:31.621738Z","iopub.status.idle":"2023-07-16T17:25:31.654397Z","shell.execute_reply.started":"2023-07-16T17:25:31.621707Z","shell.execute_reply":"2023-07-16T17:25:31.653458Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"lora_model.to(device)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-07-16T17:25:32.382373Z","iopub.execute_input":"2023-07-16T17:25:32.382764Z","iopub.status.idle":"2023-07-16T17:25:32.402155Z","shell.execute_reply.started":"2023-07-16T17:25:32.382729Z","shell.execute_reply":"2023-07-16T17:25:32.401232Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): BloomForCausalLM(\n      (transformer): BloomModel(\n        (word_embeddings): Embedding(250880, 1024)\n        (word_embeddings_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (h): ModuleList(\n          (0-23): 24 x BloomBlock(\n            (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (self_attention): BloomAttention(\n              (query_key_value): Linear(\n                in_features=1024, out_features=3072, bias=True\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (attention_dropout): Dropout(p=0.0, inplace=False)\n            )\n            (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): BloomMLP(\n              (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n              (gelu_impl): BloomGelu()\n              (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n          )\n        )\n        (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (lm_head): Linear(in_features=1024, out_features=250880, bias=False)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"prompter = Prompter()","metadata":{"execution":{"iopub.status.busy":"2023-07-16T17:25:33.771320Z","iopub.execute_input":"2023-07-16T17:25:33.771991Z","iopub.status.idle":"2023-07-16T17:25:33.776395Z","shell.execute_reply.started":"2023-07-16T17:25:33.771955Z","shell.execute_reply":"2023-07-16T17:25:33.775138Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"data_process = DataProcess(data_path = \"MBZUAI/Bactrian-X\", tokenizer = tokenizer)\ndataset = data_process.load_data()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-07-16T17:25:34.411541Z","iopub.execute_input":"2023-07-16T17:25:34.412295Z","iopub.status.idle":"2023-07-16T17:25:37.191280Z","shell.execute_reply.started":"2023-07-16T17:25:34.412255Z","shell.execute_reply":"2023-07-16T17:25:37.190321Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# data_process.draw(data_process.statistical(dataset, prompter))","metadata":{"execution":{"iopub.status.busy":"2023-07-16T17:19:39.749757Z","iopub.execute_input":"2023-07-16T17:19:39.750194Z","iopub.status.idle":"2023-07-16T17:19:39.754689Z","shell.execute_reply.started":"2023-07-16T17:19:39.750156Z","shell.execute_reply":"2023-07-16T17:19:39.753617Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"splited_dataset = dataset.train_test_split(test_size = 0.1, seed = 42)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T17:25:38.571282Z","iopub.execute_input":"2023-07-16T17:25:38.571652Z","iopub.status.idle":"2023-07-16T17:25:38.900944Z","shell.execute_reply.started":"2023-07-16T17:25:38.571621Z","shell.execute_reply":"2023-07-16T17:25:38.899808Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"splited_dataset","metadata":{"execution":{"iopub.status.busy":"2023-07-16T17:25:39.394464Z","iopub.execute_input":"2023-07-16T17:25:39.395285Z","iopub.status.idle":"2023-07-16T17:25:39.404557Z","shell.execute_reply.started":"2023-07-16T17:25:39.395243Z","shell.execute_reply":"2023-07-16T17:25:39.403293Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['instruction', 'input', 'id', 'output'],\n        num_rows: 60315\n    })\n    test: Dataset({\n        features: ['instruction', 'input', 'id', 'output'],\n        num_rows: 6702\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"model_inputs = MODEL_INPUTS(prompter = prompter,\n                            tokenizer = tokenizer,\n                            max_length = 512)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T17:25:40.101906Z","iopub.execute_input":"2023-07-16T17:25:40.102275Z","iopub.status.idle":"2023-07-16T17:25:40.107600Z","shell.execute_reply.started":"2023-07-16T17:25:40.102246Z","shell.execute_reply":"2023-07-16T17:25:40.106113Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"train_data = splited_dataset[\"train\"].shuffle().map(model_inputs.generate_and_tokenize_prompt)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T17:25:40.809778Z","iopub.execute_input":"2023-07-16T17:25:40.810236Z","iopub.status.idle":"2023-07-16T17:26:47.572931Z","shell.execute_reply.started":"2023-07-16T17:25:40.810202Z","shell.execute_reply":"2023-07-16T17:26:47.572005Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/60315 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}]},{"cell_type":"code","source":"train_data = train_data.remove_columns([\"instruction\", \"input\", \"id\", \"output\"])","metadata":{"execution":{"iopub.status.busy":"2023-07-16T17:26:47.575034Z","iopub.execute_input":"2023-07-16T17:26:47.575545Z","iopub.status.idle":"2023-07-16T17:26:47.586254Z","shell.execute_reply.started":"2023-07-16T17:26:47.575477Z","shell.execute_reply":"2023-07-16T17:26:47.585284Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"valid_data = splited_dataset[\"test\"].map(model_inputs.generate_and_tokenize_prompt)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T17:26:47.587864Z","iopub.execute_input":"2023-07-16T17:26:47.588465Z","iopub.status.idle":"2023-07-16T17:26:55.838055Z","shell.execute_reply.started":"2023-07-16T17:26:47.588430Z","shell.execute_reply":"2023-07-16T17:26:55.837032Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6702 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}]},{"cell_type":"code","source":"valid_data = valid_data.remove_columns([\"instruction\", \"input\", \"id\", \"output\"])","metadata":{"execution":{"iopub.status.busy":"2023-07-16T17:26:55.841007Z","iopub.execute_input":"2023-07-16T17:26:55.841788Z","iopub.status.idle":"2023-07-16T17:26:55.851676Z","shell.execute_reply.started":"2023-07-16T17:26:55.841749Z","shell.execute_reply":"2023-07-16T17:26:55.850718Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"train_data.set_format(\"torch\")\nvalid_data.set_format(\"torch\")","metadata":{"execution":{"iopub.status.busy":"2023-07-16T17:26:55.853314Z","iopub.execute_input":"2023-07-16T17:26:55.853765Z","iopub.status.idle":"2023-07-16T17:26:55.892608Z","shell.execute_reply.started":"2023-07-16T17:26:55.853730Z","shell.execute_reply":"2023-07-16T17:26:55.891492Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"train_dataloader, valid_dataloader = model_inputs.prepare_dataloader(train_data, valid_data, batch_size = 2)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T17:26:55.894359Z","iopub.execute_input":"2023-07-16T17:26:55.894910Z","iopub.status.idle":"2023-07-16T17:26:55.903585Z","shell.execute_reply.started":"2023-07-16T17:26:55.894858Z","shell.execute_reply":"2023-07-16T17:26:55.902516Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"metrics = evaluate.load(\"rouge\")","metadata":{"execution":{"iopub.status.busy":"2023-07-16T17:26:55.905235Z","iopub.execute_input":"2023-07-16T17:26:55.905696Z","iopub.status.idle":"2023-07-16T17:26:57.985859Z","shell.execute_reply.started":"2023-07-16T17:26:55.905661Z","shell.execute_reply":"2023-07-16T17:26:57.984843Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c137632b5664962847fc78941ea3281"}},"metadata":{}}]},{"cell_type":"code","source":"evalntest = EVALUATEandTEST(tokenizer = tokenizer,\n                            metrics = metrics,\n                            device = device,\n                            prompter = prompter,\n                            ctx = ctx)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T17:26:57.987215Z","iopub.execute_input":"2023-07-16T17:26:57.987573Z","iopub.status.idle":"2023-07-16T17:26:57.993475Z","shell.execute_reply.started":"2023-07-16T17:26:57.987539Z","shell.execute_reply":"2023-07-16T17:26:57.992120Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(lr = 1e-4,\n                  epochs = 5,\n                  model = lora_model,\n                  gradient_accumulation_steps = 4,\n                  device = device,\n                  evaluate_fn = evalntest.evaluate,\n                  mixed_precision_dtype = mixed_precision_dtype,\n                  scaler = scaler, \n                  ctx = ctx)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T17:26:57.995305Z","iopub.execute_input":"2023-07-16T17:26:57.996150Z","iopub.status.idle":"2023-07-16T17:26:58.007342Z","shell.execute_reply.started":"2023-07-16T17:26:57.996115Z","shell.execute_reply":"2023-07-16T17:26:58.006384Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"trainer.train(train_dataloader = train_dataloader,\n              display_steps = 500,\n              save_steps = 3000,\n              save_name = \"bloom-560m-checkpoint.pt\",\n              valid_dataloader = valid_dataloader,\n              samples_gen = 100,\n              samples_eval = None,\n              gen_mode = False,\n              save_checkpoint = True,\n              checkpoint = None)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T17:26:58.011363Z","iopub.execute_input":"2023-07-16T17:26:58.011636Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/150790 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d86e642dd2bb41adad1f6f7f022f2c6f"}},"metadata":{}},{"name":"stderr","text":"You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 1 -- step: 500 -- train_loss: 2.4628809394836426 -- val_loss: 1.7886342240674786\nEpoch: 1 -- step: 1000 -- train_loss: 2.113223148941994 -- val_loss: 1.7186852011777365\nEpoch: 1 -- step: 1500 -- train_loss: 1.9838043514490127 -- val_loss: 1.7006095085916715\nEpoch: 1 -- step: 2000 -- train_loss: 1.9104936387240887 -- val_loss: 1.6874646208635624\nEpoch: 1 -- step: 2500 -- train_loss: 1.8636505910634995 -- val_loss: 1.677755925592043\nEpoch: 1 -- step: 3000 -- train_loss: 1.8314305991927782 -- val_loss: 1.6700203587175733\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
